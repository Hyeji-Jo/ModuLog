# 성능 비교 항목
- loss/val/segmentation
    - 검증 데이터에서 segmentation loss - 음성 분할 관련 손실
    - 낮을수록 모델이 더 정확하게 분할됨
- loss/val/vad
    - 검증 데이터에서 음성 활동 감지 (VAD) loss 값
- loss/val
    - 전체 검증 loss 값
    - 작을수록 모델이 더 정확함
- DiarizationErrorRate
    - 전체 화자인식 오류율(DER)
    - 작을수록 좋음
- DiarizationErrorRate/Confusion
    - 화자 간 혼동률 - 잘못된 화자 매칭 비율
    - 낮을수록 좋음
- DiarizationErrorRate/FalseAlarm
    - 음성이 없는데 있다고 감지한 비율
    - 낮을수록 좋음
- DiarizationErrorRate/Miss
    - 놓친 음성 비율
    - 낮을수록 좋음
- DiarizationErrorRate/Threshold
    - DER 계산 시 사용된 임계값

## 1) Adam + 20epoch
- Momentum + adaptive learning rate
- 빠른 수렴의 장점이 있지만, 과적합 위험 존재

|  Epoch | loss/val/segmentation | loss/val/vad | loss/val | DiarizationErrorRate | Confusion | FalseAlarm | Miss | Threshold |
|--------|----------------------|--------------|----------|----------------------|-----------|------------|------|-----------|
| 0      | 0.40065              | 0.488325     | 0.88895  | 0.6246               | 0.03265   | 0.05255    | 0.5398 | 0.6300    |
| 1      | 0.2192               | 0.24385      | 0.46305  | 0.2501               | 0.0352    | 0.0796     | 0.1353 | 0.6200    |
| 2      | 0.20745              | 0.2365       | 0.44385  | 0.2402               | 0.0311    | 0.0781     | 0.1311 | 0.5800    |
| 3      | 0.20395              | 0.23325      | 0.4372   | 0.2362               | 0.0297    | 0.0790     | 0.1274 | 0.5800    |
| 4      | 0.2004               | 0.2317       | 0.4321   | 0.2344               | 0.0289    | 0.0798     | 0.1258 | 0.5600    |
| 5      | 0.199                | 0.2294       | 0.42835  | 0.2310               | 0.0285    | 0.0771     | 0.1254 | 0.5600    |
| 6      | 0.198                | 0.22975      | 0.42775  | 0.2292               | 0.0278    | 0.0761     | 0.1253 | 0.5800    |
| 7      | 0.1965               | 0.2298       | 0.4263   | 0.2280               | 0.0273    | 0.0754     | 0.1253 | 0.5800    |
| 8      | 0.1957               | 0.2294       | 0.4251   | 0.2266               | 0.0268    | 0.0754     | 0.1244 | 0.5800    |
| 9      | 0.1942               | 0.2275       | 0.42175  | 0.2252               | 0.0265    | 0.0732     | 0.1256 | 0.5800    |
| 10     | 0.19305              | 0.22785      | 0.4209   | 0.2242               | 0.0260    | 0.0757     | 0.1225 | 0.5800    |
| 11     | 0.19265              | 0.22605      | 0.41865  | 0.2237               | 0.0264    | 0.0743     | 0.1230 | 0.5600    |
| 12     | 0.1926               | 0.226        | 0.41855  | 0.2227               | 0.0260    | 0.0738     | 0.1229 | 0.5800    |
| 13     | 0.1914               | 0.22605      | 0.41745  | 0.2223               | 0.0257    | 0.0727     | 0.1239 | 0.5800    |
| 14     | 0.18955              | 0.22485      | 0.41445  | 0.2217               | 0.0252    | 0.0777     | 0.1188 | 0.5600    |
| 15     | 0.189                | 0.22365      | 0.41265  | 0.2207               | 0.0253    | 0.0746     | 0.1208 | 0.5600    |
| 16     | 0.18815              | 0.223        | 0.4112   | 0.2196               | 0.0249    | 0.0768     | 0.1180 | 0.5600    |
| 17     | 0.18885              | 0.2245       | 0.4133   | 0.2191               | 0.0249    | 0.0759     | 0.1183 | 0.5800    |
| 18     | 0.1892               | 0.22435      | 0.4136   | 0.2191               | 0.0249    | 0.0729     | 0.1213 | 0.5800    |
| 19     | 0.18755              | 0.22245      | 0.41     | 0.2181               | 0.0244    | 0.0755     | 0.1182 | 0.5600    |


## 2) AdamW + 20epoch
- Adam + Decoupled Weight Decay
- L2 정규화로 일반화 성능이 향상 되지만 학습률 튜닝에 민감  
  
|    |   loss/val/segmentation |   loss/val/vad |   loss/val |   DiarizationErrorRate |   DiarizationErrorRate/Confusion |   DiarizationErrorRate/FalseAlarm |   DiarizationErrorRate/Miss |   DiarizationErrorRate/Threshold |   loss/train/segmentation |   loss/train/vad |   loss/train |
|----|-------------------------|----------------|------------|------------------------|----------------------------------|-----------------------------------|-----------------------------|----------------------------------|---------------------------|------------------|--------------|
|  0 |                 0.39935 |       0.431175 |    0.8305  |                0.39435 |                          0.03865 |                           0.09995 |                      0.2557 |                             0.57 |                 nan       |        nan       |    nan       |
|  1 |                 0.2193  |       0.24265  |    0.46195 |                0.2545  |                          0.0359  |                           0.0849  |                      0.1337 |                             0.6  |                   0.28875 |          0.2774  |      0.56615 |
|  2 |                 0.21015 |       0.23805  |    0.44815 |                0.2464  |                          0.0329  |                           0.081   |                      0.1326 |                             0.58 |                   0.22295 |          0.2418  |      0.46475 |
|  3 |                 0.2104  |       0.2355   |    0.44585 |                0.2416  |                          0.0321  |                           0.0812  |                      0.1283 |                             0.6  |                   0.2092  |          0.23425 |      0.44345 |
|  4 |                 0.2037  |       0.2315   |    0.4352  |                0.237   |                          0.0309  |                           0.0789  |                      0.1273 |                             0.58 |                   0.2024  |          0.2298  |      0.4322  |
|  5 |                 0.19995 |       0.2296   |    0.42955 |                0.2339  |                          0.0293  |                           0.0797  |                      0.1248 |                             0.56 |                   0.1976  |          0.2252  |      0.4228  |
|  6 |                 0.1999  |       0.22815  |    0.428   |                0.2314  |                          0.0293  |                           0.0732  |                      0.1289 |                             0.6  |                   0.1934  |          0.2227  |      0.41615 |
|  7 |                 0.1986  |       0.22815  |    0.42675 |                0.231   |                          0.028   |                           0.0756  |                      0.1274 |                             0.6  |                   0.19045 |          0.22025 |      0.4107  |
|  8 |                 0.197   |       0.2268   |    0.42385 |                0.2294  |                          0.0276  |                           0.0748  |                      0.127  |                             0.6  |                   0.1875  |          0.21715 |      0.40465 |
|  9 |                 0.1953  |       0.22535  |    0.42075 |                0.2274  |                          0.0272  |                           0.0773  |                      0.123  |                             0.58 |                   0.1852  |          0.21555 |      0.4008  |
| 10 |                 0.1938  |       0.22505  |    0.4188  |                0.2267  |                          0.0268  |                           0.0754  |                      0.1244 |                             0.58 |                   0.18315 |          0.21395 |      0.39715 |
| 11 |                 0.1919  |       0.2245   |    0.4164  |                0.2255  |                          0.0266  |                           0.0725  |                      0.1263 |                             0.58 |                   0.18135 |          0.21225 |      0.3936  |
| 12 |                 0.1915  |       0.2241   |    0.41555 |                0.2241  |                          0.0261  |                           0.0767  |                      0.1214 |                             0.58 |                   0.17985 |          0.21135 |      0.39115 |
| 13 |                 0.19125 |       0.225    |    0.4163  |                0.2241  |                          0.026   |                           0.0714  |                      0.1267 |                             0.6  |                   0.1788  |          0.2109  |      0.38975 |
| 14 |                 0.18935 |       0.2229   |    0.4122  |                0.2224  |                          0.0252  |                           0.0751  |                      0.1221 |                             0.58 |                   0.17655 |          0.20895 |      0.3855  |
| 15 |                 0.18815 |       0.2228   |    0.4109  |                0.2219  |                          0.0253  |                           0.074   |                      0.1226 |                             0.56 |                   0.1765  |          0.20885 |      0.38535 |
| 16 |                 0.18895 |       0.22285  |    0.4118  |                0.2215  |                          0.0251  |                           0.0756  |                      0.1209 |                             0.58 |                   0.17485 |          0.2074  |      0.38225 |
| 17 |                 0.1881  |       0.22225  |    0.41035 |                0.2199  |                          0.0249  |                           0.0765  |                      0.1186 |                             0.58 |                   0.1736  |          0.2059  |      0.37955 |
| 18 |                 0.18835 |       0.2233   |    0.41165 |                0.2204  |                          0.0246  |                           0.0758  |                      0.12   |                             0.58 |                   0.17325 |          0.2063  |      0.3795  |
| 19 |                 0.1878  |       0.22205  |    0.4098  |                0.2198  |                          0.0244  |                           0.0771  |                      0.1183 |                             0.58 |                   0.17225 |          0.20485 |      0.3771  |

## 3) RAdam (Rectified Adam)
- Adam의 빠른 수렴성과 SGD의 안정성을 절충
- 일반화 성능에서 Adam보다 좋다고 보고된 경우 많음
```py
from torch.optim import RAdam
def configure_optimizers(self):
    return RAdam(self.parameters(), lr=1e-4)
```



## 4) Lookahead + Adam (or RAdam)
- 두 개의 옵티마이저를 병렬로 사용해서 빠른 탐색 + 안정적 수렴
```py
pip install lookahead_pytorch

from lookahead import Lookahead
from torch.optim import RAdam

def configure_optimizers(self):
    base_opt = RAdam(self.parameters(), lr=1e-4)
    return Lookahead(base_opt, k=5, alpha=0.5)

```

## 5) RAdam + CosineAnnealing
- 정규화 + 학습률 스케줄링 실험
```py
from torch.optim import RAdam
from torch.optim.lr_scheduler import CosineAnnealingLR

def configure_optimizers(self):
    optimizer = RAdam(self.parameters(), lr=1e-4)
    scheduler = CosineAnnealingLR(optimizer, T_max=20)
    return [optimizer], [scheduler]
```
