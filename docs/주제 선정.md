# 주제 후보
- 실시간 번역
- 다국어 회의
- **다국어 회의록 요약**
- AI 고객센터
- 노인 인공지능 전화 서비스

# 자료 조사
## 다중발화 음성인식
### [Speech Recognition and Multi-Speaker Diarization of Long Conversations](https://github.com/calclavia/tal-asrd)

### [Cross-Speaker Encoding Network for Multi-Talker Speech Recognition(2024)](https://github.com/kjw11/csenet-asr)
- 기존의 다중 화자 음성인식 모델의 접근법
  - SIMO(Single-Input Multiple-Output) 모델 : 여러 개의 분리된 인코더를 통해 처리
  - SISO(Single-Input Single-Output) 모델 : 주로 어템션 기반 인코더-디코더 아카텍처 사용
  - 과정 : 음성 분리 -> 화자 분리 -> 음성인식
- 본 논문은 SIMO 모델의 한계를 극복
- LibrispeechMix 데이터셋 사용
  - 훈련 데이터: 960시간
  - 검증 데이터: 10시간
  - 테스트 데이터: 10시간
  - 각 발화를 다른 화자의 발화와 혼합하여 중첩 발화(overlapping speech) 생성
  - 신호 대 잡음비(SNR, Signal-to-Noise Ratio)를 -5 dB ~ 5 dB 범위에서 설정하여 다양한 중첩 강도를 시뮬레이션 
- **CSE 모델 특징**
  - 기존 SIMO 모델의 경우 각 화자의 음성을 개별적으로 처리하지만, CSE 네트워크는 전체 발화 내의 모든 화자의 특징을 통합하여 문맥정보를 더 풍부하게 학습
  - 중첩이 많을수록 성능이 좋으며, 중첩이 0~50%로 낮은 경우 성능이 일반 모델보다 떨어짐
  - 화자 인식 인코더 -> 화자간 의존성 학습 -> 텍스트 디코더
    - 화자 인식 인코더 : Transformer 기반 구조 / CNN 기반 전처리
    - 화자간 의존성 학습 : Cross-Attention 메커니즘을 활용하여 각 화자의 임베딩을 다른 화자의 임베딩을 고려하여 학습
    - 텍스트 디코더 : Attention 기반 /  Serialized Output Training (SOT) 방식을 적용
- 본 논문에서 사용된 모델은 Conformer (Transformer + CNN )
  - Whisper의 경우 SIMO 처럼 동작하기 위해 **Pyannote-Diarization** 패키지 사용 필요
  - 본 논문을 whisper로 변경하려면 전체 입출력을 바꿔야 함
- License : 연구 및 서비스개발에 자유롭게 사용 가능
- GPT의 조언
  -  Conformer를 다국어 데이터셋으로 파인튜닝 (Fine-tuning)
  -  Conformer 모델을 그대로 사용하되, Whisper 같은 다국어 ASR 모델과 결합

### [Acoustic modeling for Overlapping Speech Recognition- JHU Chime-5 Challenge System](https://github.com/fgnt/nara_wpe)
원거리 및 중첩 음성 환경에서 음성 인식 성능을 향상시키기 위해 다양한 **데이터 증강, 신경망 모델 개선, 전처리 기법**을 적용하고 Kaldi 기반 새로운 ASR 시스템을 개발

- **환경**
    - 마이크 방향을 향하지 않은 화자도 존재→ 왜곡 있음
    - 집안의 소음이 존재
    - 다양한 위치에서 녹음

The speech data consists of real 4-people dinner party conversations recorded using linear array microphones, and annotated with speech
start/end times, the speaker labels and speech transcription.

- **데이터 증강**
    - chime-5
    - Kaldi → 채택
        - 합성 RIR 사용
        - 외부 소음 데이터 (MUSAN) 추가
- **Neural Network Architentures**
    
    : 화자분리는 X
    
    - TDNN
    - TDNN + LSTM
    - TDNN + F
    - DNN + TDNN + LSTM

### [Speaker Embedding-aware Neural Diarization: an Efficient Framework for Overlapping Speech Diarization in Meeting Scenarios(2022)](https://github.com/modelscope/FunASR)
- 회의에서 누가 언제 말을 했는가를 초점 -> SEND 프레임워크로 SD 한계 극복
- 기존 방법론의 한계
  - 클러스터링 기반 SD 기법 : 중첩 발화 처리에 어려움
  - End-to-End Neural Diarization : 라벨 순서 문제 발생 및 대규모 지도 학습 데이터 필요 -> **Conformer 기반 모델**
    - 새로운 환경에서 일반화되기 어려움
    - 학습 복잡 
  - Target-Speaker Voice Activity Detection : 성능은 우수하나 계산량이 많아 효율성 낮음
    - 실시간 시스템 적용이 어려움 
- 적용 기술
  - 화자 분리 정확도를 높이면서 계산량은 줄임
  - Power-set Encoding (PSE)
    - 다중 화자 분류 문제를 단일 라벨 예측(single-label prediction) 문제로 변환하여 학습
    - 중첩 발화 처리를 더 효과적
  - 듀얼 유사도 평가 기법(Dual Similarity Scoring Mechanism) 도입
    - Context-Independent (CI) Scorer: 현재 프레임에서 화자의 특징을 독립적으로 평가
    - Context-Dependent (CD) Scorer: 문맥(Context)을 고려하여 화자 임베딩을 정교하게 조정   
- 모델 아키텍처
  - 음성 인코더 : FSMN 모델로 긴 문맥 정보로 부터 음향 특징 추출
  - 화자 인코더 : 화자 임베딩 추출 / 사전 학습된 TDNN 활용
  - 이중 유사도 평가 기법
    - Context-Independent (CI) Scorer : 화자 임베딩 간 유사도 도출
    - Context-Dependent (CD) Scorer :  문맥을 반영한 화자 임베딩 학습
  - Power-Set Encoding (PSE) : 기존 모델들은 다중 라벨 분류지만 단일 라벨 예측 문제로 변환
  	•	예시:
  	•	화자 A, B, C가 있다고 가정
  	•	프레임 1: A만 말함 → Label: {A}
  	•	프레임 2: A와 B가 동시에 말함 → Label: {A, B}
  	•	프레임 3: 아무도 말하지 않음 → Label: {}
  	•	이러한 조합을 고유한 단일 라벨로 인코딩하여 학습 수행
- 학습 과정
  - 데이터로 기본 모델 지도 학습
  - 실제 회의 데이터로 파인튜닝
  - 도메인 적용해 최적화 
- 학습 데이터
  - AliMeeting : 실제 회의 데이터를 포함한 다중 화자 음성 데이터셋
  - 총 120시간의 녹음 데이터 포함
  - 발화 중첩 : 34~42%
- License : 연구용으로는 사용이 가능하나 서비스 개발은 조심(직접적 언급이 없음)해야하며 출처 및 모델 이름을 유지해야 함
  - 학술 연구 및 비영리적 목적으로 활용하는 것은 제한이 없음.
    - 단, 논문이나 연구 자료에서 FunASR의 출처와 저작권 정보(Alibaba Group)를 명시해야 함.

 
## 다중발화 성능
- [On Word Error Rate Definitions and their Efficient Computation for Multi-Speaker Speech Recognition Systems](https://github.com/fgnt/meeteval)
