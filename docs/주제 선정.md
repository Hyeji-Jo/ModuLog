# 주제 후보
- 실시간 번역
- 다국어 회의
- **다국어 회의록 요약**
- AI 고객센터
- 노인 인공지능 전화 서비스

# 자료 조사
## 다중발화 음성인식
### [Speech Recognition and Multi-Speaker Diarization of Long Conversations](https://github.com/calclavia/tal-asrd)

### [Cross-Speaker Encoding Network for Multi-Talker Speech Recognition(2024)](https://github.com/kjw11/csenet-asr)
- 기존의 다중 화자 음성인식 모델의 접근법
  - SIMO(Single-Input Multiple-Output) 모델 : 여러 개의 분리된 인코더를 통해 처리
  - SISO(Single-Input Single-Output) 모델 : 주로 어템션 기반 인코더-디코더 아카텍처 사용
  - 과정 : 음성 분리 -> 화자 분리 -> 음성인식
- 본 논문은 SIMO 모델의 한계를 극복
- LibrispeechMix 데이터셋 사용
  - 훈련 데이터: 960시간
  - 검증 데이터: 10시간
  - 테스트 데이터: 10시간
  - 각 발화를 다른 화자의 발화와 혼합하여 중첩 발화(overlapping speech) 생성
  - 신호 대 잡음비(SNR, Signal-to-Noise Ratio)를 -5 dB ~ 5 dB 범위에서 설정하여 다양한 중첩 강도를 시뮬레이션 
- **CSE 모델 특징**
  - 기존 SIMO 모델의 경우 각 화자의 음성을 개별적으로 처리하지만, CSE 네트워크는 전체 발화 내의 모든 화자의 특징을 통합하여 문맥정보를 더 풍부하게 학습
  - 중첩이 많을수록 성능이 좋으며, 중첩이 0~50%로 낮은 경우 성능이 일반 모델보다 떨어짐
  - 화자 인식 인코더 -> 화자간 의존성 학습 -> 텍스트 디코더
    - 화자 인식 인코더 : Transformer 기반 구조 / CNN 기반 전처리
    - 화자간 의존성 학습 : Cross-Attention 메커니즘을 활용하여 각 화자의 임베딩을 다른 화자의 임베딩을 고려하여 학습
    - 텍스트 디코더 : Attention 기반 /  Serialized Output Training (SOT) 방식을 적용
- 본 논문에서 사용된 모델은 Conformer (Transformer + CNN )
  - Whisper의 경우 SIMO 처럼 동작하기 위해 **Pyannote-Diarization** 패키지 사용 필요
  - 본 논문을 whisper로 변경하려면 전체 입출력을 바꿔야 함
- License : 연구 및 서비스개발에 자유롭게 사용 가능
- GPT의 조언
  -  Conformer를 다국어 데이터셋으로 파인튜닝 (Fine-tuning)
  -  Conformer 모델을 그대로 사용하되, Whisper 같은 다국어 ASR 모델과 결합

### [Acoustic modeling for Overlapping Speech Recognition- JHU Chime-5 Challenge System](https://github.com/fgnt/nara_wpe)
원거리 및 중첩 음성 환경에서 음성 인식 성능을 향상시키기 위해 다양한 **데이터 증강, 신경망 모델 개선, 전처리 기법**을 적용하고 Kaldi 기반 새로운 ASR 시스템을 개발

- **환경**
    - 마이크 방향을 향하지 않은 화자도 존재→ 왜곡 있음
    - 집안의 소음이 존재
    - 다양한 위치에서 녹음

The speech data consists of real 4-people dinner party conversations recorded using linear array microphones, and annotated with speech
start/end times, the speaker labels and speech transcription.

- **데이터 증강**
    - chime-5
    - Kaldi → 채택
        - 합성 RIR 사용
        - 외부 소음 데이터 (MUSAN) 추가
- **Neural Network Architentures**
    
    : 화자분리는 X
    
    - TDNN
    - TDNN + LSTM
    - TDNN + F
    - DNN + TDNN + LSTM

### [Speaker Embedding-aware Neural Diarization: an Efficient Framework for Overlapping Speech Diarization in Meeting Scenarios(2022)](https://github.com/modelscope/FunASR)
- 회의에서 누가 언제 말을 했는가를 초점 -> SEND 프레임워크로 SD 한계 극복
- 기존 방법론의 한계
  - 클러스터링 기반 SD 기법 : 중첩 발화 처리에 어려움
  - End-to-End Neural Diarization : 라벨 순서 문제 발생 및 대규모 지도 학습 데이터 필요 -> **Conformer 기반 모델**
    - 새로운 환경에서 일반화되기 어려움
    - 학습 복잡 
  - Target-Speaker Voice Activity Detection : 성능은 우수하나 계산량이 많아 효율성 낮음
    - 실시간 시스템 적용이 어려움 
- 적용 기술
  - 화자 분리 정확도를 높이면서 계산량은 줄임
  - Power-set Encoding (PSE)
    - 다중 화자 분류 문제를 단일 라벨 예측(single-label prediction) 문제로 변환하여 학습
    - 중첩 발화 처리를 더 효과적
  - 듀얼 유사도 평가 기법(Dual Similarity Scoring Mechanism) 도입
    - Context-Independent (CI) Scorer: 현재 프레임에서 화자의 특징을 독립적으로 평가
    - Context-Dependent (CD) Scorer: 문맥(Context)을 고려하여 화자 임베딩을 정교하게 조정   
- 모델 아키텍처
  - 음성 인코더 : FSMN 모델로 긴 문맥 정보로 부터 음향 특징 추출
  - 화자 인코더 : 화자 임베딩 추출 / 사전 학습된 TDNN 활용
  - 이중 유사도 평가 기법
    - Context-Independent (CI) Scorer : 화자 임베딩 간 유사도 도출
    - Context-Dependent (CD) Scorer :  문맥을 반영한 화자 임베딩 학습
  - Power-Set Encoding (PSE) : 기존 모델들은 다중 라벨 분류지만 단일 라벨 예측 문제로 변환
  	•	예시:
  	•	화자 A, B, C가 있다고 가정
  	•	프레임 1: A만 말함 → Label: {A}
  	•	프레임 2: A와 B가 동시에 말함 → Label: {A, B}
  	•	프레임 3: 아무도 말하지 않음 → Label: {}
  	•	이러한 조합을 고유한 단일 라벨로 인코딩하여 학습 수행
- 학습 과정
  - 데이터로 기본 모델 지도 학습
  - 실제 회의 데이터로 파인튜닝
  - 도메인 적용해 최적화 
- 학습 데이터
  - AliMeeting : 실제 회의 데이터를 포함한 다중 화자 음성 데이터셋
  - 총 120시간의 녹음 데이터 포함
  - 발화 중첩 : 34~42%
- License : 연구용으로는 사용이 가능하나 서비스 개발은 조심(직접적 언급이 없음)해야하며 출처 및 모델 이름을 유지해야 함
  - 학술 연구 및 비영리적 목적으로 활용하는 것은 제한이 없음.
    - 단, 논문이나 연구 자료에서 FunASR의 출처와 저작권 정보(Alibaba Group)를 명시해야 함.

 
## 다중발화 성능
- [On Word Error Rate Definitions and their Efficient Computation for Multi-Speaker Speech Recognition Systems](https://github.com/fgnt/meeteval)

## 자연어처리 텍스트 요약(Text Summarization)
### [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461)
![image](https://github.com/user-attachments/assets/68cd5b04-4c97-4f97-abd0-b30bfc65ee6d)
  - 자연어 처리에서 텍스트 요약은 긴 문서에서 핵심 내용을 추출하여 압축된 정보를 제공하는 중요한 기술
  - 기존 요약 기법들의 한계를 극복하기 위해 사전 학습된 언어 모델(Pre-trained Language Models) 및 다양한 기법을 활용


 - BART 모델의 개념 및 구조
   - BART ?
     - Bidirectional Encoder와 Auto-Regressive Decoder를 결합한 seq2seq 모델
     - 다양한 noising 전략을 적용할 수 있는 유연한 denoising autoencoder
     - 기본적인 트랜스포머 구조 + Activation으로 ReLU 대신 GeLU 사용
     - Base: 6 enc layers & 6 dec layers
     - Large: 12 enc layers & 12 dec layers
     - BERT와의 아키텍처적 차이점

   - 모델 아키텍처
     - 인코더: BERT와 유사한 양방향 Transformer
     - 디코더: GPT와 유사한 left-to-right autoregressive 모델, 단 cross-attention을 통해 인코더의 정보를 활용
     - 세부 차이: 디코더의 활성화 함수로 GeLU 사용, 파라미터 초기화 방식 차이 등

![image](https://github.com/user-attachments/assets/4e120a20-c847-4fe4-979f-8af2b286b7b3)
   
   - 사전학습(Pre-training) 단계
     - 텍스트 오염: 문장을 다양한 방법(토큰 마스킹, 삭제, 텍스트 인필링, 문장 순서 섞기, 문서 회전 등)으로 변형
     - 복원 학습: seq2seq 구조를 사용해 원본 텍스트를 복원하도록 학습

![image](https://github.com/user-attachments/assets/e8607219-7ed4-4269-951a-c3a1567053c6)

       
   - 정리
     - 구조 : GPT와 BERT의 특성을 결합한 Encoder-Decoder 구조
     - 사전 학습 : 입력 문장에 노이즈를 추가(단어 마스킹, 문장 순서 섞기 등)한 후 이를 원래 문장으로 복원하는 방식으로 학습
     - 활용 분야: 요약(Summarization), 기계 번역(Machine Translation), 텍스트 생성(Text Generation) 등
  
   - 성능
![image](https://github.com/user-attachments/assets/b722cd82-929d-445b-99ae-74747fbf28a2)

 - [Text Summarization Repo 참고](https://arxiv.org/abs/1910.13461)
   - BART
     - Transformer 기반, Seq2Seq
     - 생성 요약 (Abstractive)
     - 문맥 이해가 뛰어나고 부드러운 요약 가능
   - T5
     - Text-to-Text Transformer
     - 생성 요약 (Abstractive)
     - 다양한 NLP 작업 지원
   - PEGASUS
     - 대규모 뉴스 데이터 학습
     - 생성 요약 (Abstractive)
     - 뉴스, 논문 요약 성능 우수
   - BERTSUM
     - BERT 기반, Extractive
     - 추출 요약 (Extractive)
     - 원본 문장을 유지하여 신뢰도 높음
    
     
